[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A tidyverse user’s guide to polars library",
    "section": "",
    "text": "Preface\nThis book was inspired by a series of blog posts recommending a Pythonic data science stack that mimics the comfort and familiarity of R’s tidyverse tools. Particularly, this book explores the Polars library, a rising star in the DataFrame space (alongside established players like Pandas, Dask, Modin, Ray, and Vaex).\nIn a nutshell, Polars is defined as a query engine with a DataFrame front-end. It offers a rich set of intuitive functions and principled workflows for data manipulation and analysis. Designed from the ground up with performance in mind, Polars is also noted for its lightning-fast execution speed. Polars, despite its quick rise in popularity, is still in its early stages of development.\nThe majority of this book contains structured examples of data wrangling tasks that demonstrates idiomatic Polars and related dplyr/tidyr code for comparison. Examples might include discussion on API choices.\nCode\nimport polars as pl\nfrom lets_plot import *\nLetsPlot.setup_html(isolated_frame=False, offline=True, no_js=True, show_status=False)\n\nstars = (\n    pl.read_csv('./data/star-history-2024215.csv', has_header=False)\n    .rename({'column_1': 'project', 'column_2': 'date', 'column_3': 'stars'})\n    .with_columns(\n        pl.col('date').str.slice(0, 15).str.to_date('%a %b %d %Y').alias('short_date')\n    )\n    .sort('short_date')\n)\n\n(\n    ggplot(stars.to_pandas(), aes(x='short_date', y='stars', color='project')) +\n    geom_line(size=1.5) +\n    labs(y='Github stars', title='Polars is surging in popularity') +\n    scale_x_datetime(name='Date') +\n    theme(legend_title=element_blank(), \n          text=element_text(family='Roboto Condensed', size=15),\n         plot_title=element_text(face='bold', size=18)) +\n    scale_y_continuous(breaks=[0, 10_000, 20_000, 30_000, 40_000], format='0,', expand=[0.01,0.01]) +\n    ggsize(700,380)\n)\n\n\n\n  \n  \n    \n    \n    \n      \n        \n        \n        \n        \n      \n      \n        \n          \n          \n          \n            \n              2010\n            \n          \n        \n        \n          \n          \n          \n            \n              2015\n            \n          \n        \n        \n          \n          \n          \n            \n              2020\n            \n          \n        \n        \n        \n      \n      \n        \n        \n        \n        \n        \n        \n        \n        \n      \n      \n        \n          \n            \n              0\n            \n          \n        \n        \n          \n            \n              10,000\n            \n          \n        \n        \n          \n            \n              20,000\n            \n          \n        \n        \n          \n            \n              30,000\n            \n          \n        \n        \n          \n            \n              40,000\n            \n          \n        \n      \n      \n        \n          \n            \n            \n          \n        \n        \n          \n          \n        \n        \n          \n          \n        \n        \n          \n          \n        \n        \n          \n          \n        \n        \n          \n          \n        \n      \n    \n    \n      \n        Polars is surging in popularity\n      \n    \n    \n      \n        Github stars\n      \n    \n    \n      \n        Date\n      \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n                \n                  \n                  \n                \n              \n              \n              \n            \n            \n              \n                pandas-dev/pandas\n              \n            \n          \n          \n            \n              \n              \n              \n                \n                  \n                  \n                \n              \n              \n              \n            \n            \n              \n                dask/dask\n              \n            \n          \n          \n            \n              \n              \n              \n                \n                  \n                  \n                \n              \n              \n              \n            \n            \n              \n                vaexio/vaex\n              \n            \n          \n          \n            \n              \n              \n              \n                \n                  \n                  \n                \n              \n              \n              \n            \n            \n              \n                modin-project/modin\n              \n            \n          \n          \n            \n              \n              \n              \n                \n                  \n                  \n                \n              \n              \n              \n            \n            \n              \n                pola-rs/polars",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#run-the-code-from-this-book",
    "href": "index.html#run-the-code-from-this-book",
    "title": "A tidyverse user’s guide to polars library",
    "section": "Run the code from this book",
    "text": "Run the code from this book\n\nClone the book repository:\n\ngit clone https://github.com/chuvanan/cookbook-python-polars.git python-polars-cookbook\ncd python-polars-cookbook\npython3 -m venv env\nsource env/bin/activate\n\nYou can install the exact packages that the book uses with the requirements.txt file:\n\npython3 -m pip install -r requirements.txt\n\nDownload the data (source: Reporting carrier on-time performance)\n\npython download-data.py",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "A tidyverse user’s guide to polars library",
    "section": "Credit",
    "text": "Credit\nThis work builds upon the organizational style of https://ddotta.github.io/cookbook-rpolars/ but the content and examples are tailored to Polars library in Python. So credit goes to Damien Dotta, all remaining errors are mine.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "A tidyverse user’s guide to polars library",
    "section": "Contributing",
    "text": "Contributing\nFeel free to open an issue if you notice any problems with this book. It’s free and open source, and your feedback is valuable to me.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "first_steps.html",
    "href": "first_steps.html",
    "title": "1  First steps with Polars",
    "section": "",
    "text": "1.1 Installation\nLet’s kick off our journey into the world of data manipulation with the Polars library. First things first, we need to set up a virtual environment for a clean installation. Install the latest stable version with:\nDepending on your use case, you might want to install the optional dependencies as well:\nTo gain insights into the installed Polars package, including version details and enabled features, utilize the show_versions() function:\nimport polars as pl\npl.show_versions()\n\n--------Version info---------\nPolars:               0.20.9\nIndex type:           UInt32\nPlatform:             Linux-5.15.0-94-generic-x86_64-with-glibc2.35\nPython:               3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n\n----Optional dependencies----\nadbc_driver_manager:  &lt;not installed&gt;\ncloudpickle:          &lt;not installed&gt;\nconnectorx:           &lt;not installed&gt;\ndeltalake:            &lt;not installed&gt;\nfsspec:               &lt;not installed&gt;\ngevent:               &lt;not installed&gt;\nhvplot:               &lt;not installed&gt;\nmatplotlib:           &lt;not installed&gt;\nnumpy:                1.26.3\nopenpyxl:             &lt;not installed&gt;\npandas:               2.2.0\npyarrow:              15.0.0\npydantic:             &lt;not installed&gt;\npyiceberg:            &lt;not installed&gt;\npyxlsb:               &lt;not installed&gt;\nsqlalchemy:           &lt;not installed&gt;\nxlsx2csv:             &lt;not installed&gt;\nxlsxwriter:           &lt;not installed&gt;",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps with Polars</span>"
    ]
  },
  {
    "objectID": "first_steps.html#installation",
    "href": "first_steps.html#installation",
    "title": "1  First steps with Polars",
    "section": "",
    "text": "pip install polars\n\npip install 'polars[numpy,pandas,pyarrow]'",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps with Polars</span>"
    ]
  },
  {
    "objectID": "first_steps.html#initial-exploration",
    "href": "first_steps.html#initial-exploration",
    "title": "1  First steps with Polars",
    "section": "1.2 Initial exploration",
    "text": "1.2 Initial exploration\nFor better presentation, this book use Polars’ configuration options to adjust the layout of printed tables. Specifically:\n\nLimit the number of columns and rows displayed in DataFrames to 10 each\nApply stylish formatting to the tables for pleasant reading experience\n\n\npl.Config.set_tbl_cols(10)\npl.Config.set_tbl_rows(10)\npl.Config.set_tbl_formatting(rounded_corners=True)\n\npolars.config.Config\n\n\n\n1.2.1 Data loading\nReading a CSV file using Polars is straightforward. Let’s take a quick look:\n\nflights = pl.read_csv('./data/flights.csv')\n\nFor those familiar with R’s dplyr, a similar method called glimpse() is available:\n\n# Glimpse the first 10 columns\nflights[:,0:10].glimpse()\n\nRows: 999\nColumns: 10\n$ Year                        &lt;i64&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022\n$ Quarter                     &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ Month                       &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ DayofMonth                  &lt;i64&gt; 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n$ DayOfWeek                   &lt;i64&gt; 5, 6, 7, 1, 2, 3, 4, 5, 6, 7\n$ FlightDate                  &lt;str&gt; '2022-01-14', '2022-01-15', '2022-01-16', '2022-01-17', '2022-01-18', '2022-01-19', '2022-01-20', '2022-01-21', '2022-01-22', '2022-01-23'\n$ Reporting_Airline           &lt;str&gt; 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX'\n$ DOT_ID_Reporting_Airline    &lt;i64&gt; 20452, 20452, 20452, 20452, 20452, 20452, 20452, 20452, 20452, 20452\n$ IATA_CODE_Reporting_Airline &lt;str&gt; 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX'\n$ Tail_Number                 &lt;str&gt; 'N119HQ', 'N122HQ', 'N412YX', 'N405YX', 'N420YX', 'N446YX', 'N116HQ', 'N419YX', 'N137HQ', 'N110HQ'\n\n\n\n\nStandard commands from pandas such as head(), tail(), and describe() can be used seamlessly:\n\nflights.head()\n\n\nshape: (5, 110)\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\n…\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\ni64\ni64\ni64\ni64\ni64\n…\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n2022\n1\n1\n14\n5\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n15\n6\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n16\n7\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n17\n1\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n18\n2\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\nflights.tail()\n\n\nshape: (5, 110)\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\n…\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\ni64\ni64\ni64\ni64\ni64\n…\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n2022\n1\n1\n12\n3\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n13\n4\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n14\n5\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n17\n1\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n18\n2\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\nflights.describe()\n\n\nshape: (9, 111)\n\n\n\nstatistic\nYear\nQuarter\nMonth\nDayofMonth\n…\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\nstr\nf64\nf64\nf64\nf64\n…\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"count\"\n999.0\n999.0\n999.0\n999.0\n…\n\"0\"\n\"0\"\n\"999\"\n\"999\"\n\"0\"\n\n\n\"null_count\"\n0.0\n0.0\n0.0\n0.0\n…\n\"999\"\n\"999\"\n\"0\"\n\"0\"\n\"999\"\n\n\n\"mean\"\n2022.0\n1.0\n1.0\n16.2002\n…\nnull\nnull\nnull\nnull\nnull\n\n\n\"std\"\n0.0\n0.0\n0.0\n8.802666\n…\nnull\nnull\nnull\nnull\nnull\n\n\n\"min\"\n2022.0\n1.0\n1.0\n1.0\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\"25%\"\n2022.0\n1.0\n1.0\n9.0\n…\nnull\nnull\nnull\nnull\nnull\n\n\n\"50%\"\n2022.0\n1.0\n1.0\n16.0\n…\nnull\nnull\nnull\nnull\nnull\n\n\n\"75%\"\n2022.0\n1.0\n1.0\n24.0\n…\nnull\nnull\nnull\nnull\nnull\n\n\n\"max\"\n2022.0\n1.0\n1.0\n31.0\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\nIf you want to take a peek at different parts of your DataFrame, here’s a handy trick: use the sample() method. This method randomly picks n number of rows from the DataFrame and returns them for inspection.\n\nflights.sample(3)\n\n\nshape: (3, 110)\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\n…\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\ni64\ni64\ni64\ni64\ni64\n…\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n2022\n1\n1\n27\n4\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n10\n1\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n13\n4\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\nThe output from Polars comes with some useful features:\n\nUnderneath each column name is a data type.\nNo index numbers are present.\nString values are quoted with double quotes.\nMissing values are represented as null, applicable to all data types.\n\n\n\n1.2.2 Row and column counting\nDetermining the number of rows and columns in a Polars DataFrame is as simple as checking the shape:\n\nflights.shape\n\n(999, 110)\n\n\n\n\n1.2.3 Converting from pandas\nTransitioning from a Pandas DataFrame to a Polars DataFrame is effortless with the from_pandas() method:\n\n\nTo clarify, cheap conversion from Polars to Pandas is achievable given the following prerequisites:\n\nPandas version 2.0 or later\nSupported by the pyarrow library\n\n\nimport pandas as pd\nflights2 = pl.from_pandas(pd.read_csv('./data/flights.csv'))\n\nflights2[:,0:9].glimpse()\n\nRows: 999\nColumns: 9\n$ Year                        &lt;i64&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022\n$ Quarter                     &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ Month                       &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ DayofMonth                  &lt;i64&gt; 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n$ DayOfWeek                   &lt;i64&gt; 5, 6, 7, 1, 2, 3, 4, 5, 6, 7\n$ FlightDate                  &lt;str&gt; '2022-01-14', '2022-01-15', '2022-01-16', '2022-01-17', '2022-01-18', '2022-01-19', '2022-01-20', '2022-01-21', '2022-01-22', '2022-01-23'\n$ Reporting_Airline           &lt;str&gt; 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX'\n$ DOT_ID_Reporting_Airline    &lt;i64&gt; 20452, 20452, 20452, 20452, 20452, 20452, 20452, 20452, 20452, 20452\n$ IATA_CODE_Reporting_Airline &lt;str&gt; 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX', 'YX'\n\n\n\n\n\n1.2.4 Understanding data structure\nThe fundamental data structures in Polars are Series and DataFrames:\n\nSeries is 1-dimensional data structure, akin to R’s atomic vector, where all elements must share the same data type.\n\n\n# Create a named Series\ns = pl.Series('a', [1, 2, 3, 2, 5])\ns\n\n\nshape: (5,)\n\n\n\na\n\n\ni64\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n5\n\n\n\n\n\n\n\n# Note that dtype of `s` is automatically inferred as Int64\ns.dtype\n\nInt64\n\n\nConstructing a Series with a specfic dtype:\n\ns2 = pl.Series('a', [1, 2, 3], dtype=pl.Float32)\ns2\n\n\nshape: (3,)\n\n\n\na\n\n\nf32\n\n\n\n\n1.0\n\n\n2.0\n\n\n3.0\n\n\n\n\n\n\n\nSeries provides a wide range of methods for various operations, including standard statistical functions like .max(), .mean(), as well as specialized ones such as .entropy() and .unique_counts().\n\n\nprint(s.max())\nprint(s.mean())\n\n5\n2.6\n\n\n\ns.unique_counts()\n\n\nshape: (4,)\n\n\n\na\n\n\nu32\n\n\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n\n\n\n\n\nDataFrames are 2-dimensional structures, similar to R’s data.frame, built on top of Series. In the examples below, borrowed from RealPython, we create DataFrame using two different approaches:\n\nCreating DataFrame from a dictionary:\n\nimport numpy as np\nnum_rows = 5000\nrng = np.random.default_rng(seed=7)\n\nbuildings_data = {\n     \"sqft\": rng.exponential(scale=1000, size=num_rows),\n     \"year\": rng.integers(low=1995, high=2023, size=num_rows),\n     \"building_type\": rng.choice([\"A\", \"B\", \"C\"], size=num_rows),\n }\nbuildings = pl.DataFrame(buildings_data)\nbuildings\n\n\nshape: (5_000, 3)\n\n\n\nsqft\nyear\nbuilding_type\n\n\nf64\ni64\nstr\n\n\n\n\n707.529256\n1996\n\"C\"\n\n\n1025.203348\n2020\n\"C\"\n\n\n568.548657\n2012\n\"A\"\n\n\n895.109864\n2000\n\"A\"\n\n\n206.532754\n2011\n\"A\"\n\n\n…\n…\n…\n\n\n710.435755\n2003\n\"C\"\n\n\n408.872783\n2009\n\"C\"\n\n\n57.562059\n2019\n\"C\"\n\n\n3728.088949\n2020\n\"C\"\n\n\n686.678345\n2011\n\"C\"\n\n\n\n\n\n\nCreating DataFrame from multiple Series:\n\ns1 = pl.Series('sqrf', rng.exponential(scale=1000, size=num_rows))\ns2 = pl.Series('year', rng.integers(low=1995, high=2023, size=num_rows))\ns3 = pl.Series('building_type', rng.choice([\"A\", \"B\", \"C\"], size=num_rows))\n\nbuildings2 = pl.DataFrame([s1, s2, s3])\nbuildings2\n\n\nshape: (5_000, 3)\n\n\n\nsqrf\nyear\nbuilding_type\n\n\nf64\ni64\nstr\n\n\n\n\n220.644811\n1998\n\"C\"\n\n\n966.183262\n2006\n\"C\"\n\n\n295.737178\n2010\n\"A\"\n\n\n233.546019\n2009\n\"C\"\n\n\n2392.394417\n2022\n\"B\"\n\n\n…\n…\n…\n\n\n373.652761\n2011\n\"A\"\n\n\n384.053786\n2010\n\"B\"\n\n\n1388.573406\n1999\n\"A\"\n\n\n1225.981395\n2007\n\"B\"\n\n\n1206.351218\n2022\n\"B\"\n\n\n\n\n\n\n\nDataFrames come with several attributes for exploration:\n\n# Get rows number\nflights.height\n\n999\n\n\n\n# Get columns number\nflights.width\n\n110\n\n\n\n# Get a list of column names\nflights.columns[:10]\n\n['Year',\n 'Quarter',\n 'Month',\n 'DayofMonth',\n 'DayOfWeek',\n 'FlightDate',\n 'Reporting_Airline',\n 'DOT_ID_Reporting_Airline',\n 'IATA_CODE_Reporting_Airline',\n 'Tail_Number']\n\n\n\n# Get a list of column dtype\nflights.dtypes[:10]\n\n[Int64, Int64, Int64, Int64, Int64, String, String, Int64, String, String]\n\n\n\n# Get a dict of columns and their dtype\nflights[:,:10].schema\n\nOrderedDict([('Year', Int64),\n             ('Quarter', Int64),\n             ('Month', Int64),\n             ('DayofMonth', Int64),\n             ('DayOfWeek', Int64),\n             ('FlightDate', String),\n             ('Reporting_Airline', String),\n             ('DOT_ID_Reporting_Airline', Int64),\n             ('IATA_CODE_Reporting_Airline', String),\n             ('Tail_Number', String)])",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps with Polars</span>"
    ]
  },
  {
    "objectID": "first_steps.html#summary",
    "href": "first_steps.html#summary",
    "title": "1  First steps with Polars",
    "section": "1.3 Summary",
    "text": "1.3 Summary\nAt first glance, Polars offers the ease of use reminiscent of R, blending with the familiarity of pandas (minus any potential frustrations).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps with Polars</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html",
    "href": "data_manipulation.html",
    "title": "2  Data manipulation",
    "section": "",
    "text": "2.1 Data type conversion\nIn R, you work with four main types of vectors: logical, integer, double, and character. When you mix different types, they get automatically adjusted in a specific order: character → double → integer → logical. If you want to intentionally adjust them, you can use functions like as.integer(), as.double(), as.logical(), and as.character().\nOn the other hand, in polars, there’s a wider variety of data types, and you need to purposefully adjust them when needed.\nfull_flights['Year'].dtype\n\nInt64\nfull_flights = full_flights.with_columns(\n    pl.col('Year').cast(pl.Utf8)\n)\nfull_flights['Year'].dtype\n\nString",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#introduction-to-methods",
    "href": "data_manipulation.html#introduction-to-methods",
    "title": "2  Data manipulation",
    "section": "2.2 Introduction to methods",
    "text": "2.2 Introduction to methods",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#single-table-operation",
    "href": "data_manipulation.html#single-table-operation",
    "title": "2  Data manipulation",
    "section": "2.3 Single table operation",
    "text": "2.3 Single table operation\n\n2.3.1 Filtering rows\n\nRetrieving all flights from Oihio (filtering based on one condition)\n\n\n# recommended way\nflights_from_ohio = full_flights.filter(pl.col('OriginStateName') == 'Ohio')\n\n# alternative way (not recommended)\n# flights_from_ohio = full_flights[full_flights['OriginStateName'] == 'Ohio']\n\nflights_from_ohio['OriginStateName'].value_counts()\n\n\nshape: (1, 2)\n\n\n\nOriginStateName\ncount\n\n\nstr\nu32\n\n\n\n\n\"Ohio\"\n7434\n\n\n\n\n\n\n\nObtaining all flights from Ohio to Virginia (filtering based on multiple conditions)\n\n\n# note that each predicate must be enclosed within parentheses\nflights_from_ohio_to_virginia = full_flights.filter(\n    (pl.col('OriginStateName') == 'Ohio') & (pl.col('DestStateName') == 'Virginia')\n)\n\n# you can replace `&` with `,` to avoid parentheses\n# flights_from_ohio_to_virginia = full_flights.filter(\n#     pl.col('OriginStateName') == 'Ohio', \n#     pl.col('DestStateName') == 'Virginia'\n# )\n\nflights_from_ohio_to_virginia['DestStateName'].value_counts()\n\n\nshape: (1, 2)\n\n\n\nDestStateName\ncount\n\n\nstr\nu32\n\n\n\n\n\"Virginia\"\n564\n\n\n\n\n\n\n\nFetching all flights from Ohito to any state except Virginia (filtering based on negative condition)\n\n\nflights_from_ohio_except_to_virginia = full_flights.filter(\n    pl.col('OriginStateName') == 'Ohio', \n    ~(pl.col('DestStateName') == 'Virginia')\n)\nflights_from_ohio_except_to_virginia['DestStateName'].value_counts(sort=True).head()\n\n\nshape: (5, 2)\n\n\n\nDestStateName\ncount\n\n\nstr\nu32\n\n\n\n\n\"Florida\"\n1349\n\n\n\"New York\"\n988\n\n\n\"Illinois\"\n617\n\n\n\"North Carolina…\n564\n\n\n\"Georgia\"\n526\n\n\n\n\n\n\n\nWhy using the .filter() method? The reason is to leverage lazy execution and query optimization\n\n\n%%time\n# this code is optimized by polars query engine before reading any data into memory\nlazy_stmt = (\n    pl.scan_csv('./data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv')\n    .filter(pl.col('OriginStateName') == 'Ohio')\n    .collect()\n)\n\nCPU times: user 3.74 s, sys: 423 ms, total: 4.16 s\nWall time: 561 ms\n\n\n\n%%time\neager_stmt = (\n    pl.read_csv('./data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv')\n    .filter(pl.col('OriginStateName') == 'Ohio')\n)\n\nCPU times: user 3.71 s, sys: 701 ms, total: 4.41 s\nWall time: 645 ms\n\n\n\n\n2.3.2 Selecting columns\nSelecting variables within a DataFrame in polars is highly expressive, offering flexibility based on either variable names or types. Here’s a breakdown from simple to complex:\n\nUtilizing the select function to create a DataFrame from Series:\n\n\nfoo = pl.Series('foo', [1, 2, 3])\nbar = pl.Series('bar', [4, 5, 6])\n\npl.select(mmin = pl.min_horizontal(foo, bar))\n\n\nshape: (3, 1)\n\n\n\nmmin\n\n\ni64\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n\nSelecting columns by names:\n\n\nfull_flights.select(['FlightDate', 'Tail_Number']).head(3)\n# or \n# full_flights.select(pl.col(['FlightDate', 'Tail_Number'])).head(3)\n\n\nshape: (3, 2)\n\n\n\nFlightDate\nTail_Number\n\n\nstr\nstr\n\n\n\n\n\"2022-01-14\"\n\"N119HQ\"\n\n\n\"2022-01-15\"\n\"N122HQ\"\n\n\n\"2022-01-16\"\n\"N412YX\"\n\n\n\n\n\n\n\nSelecting columns by types:\n\n\n# select all integer columns\nfull_flights.select(pl.col(pl.Int64)).head(3)\n\n\nshape: (3, 22)\n\n\n\nQuarter\nMonth\nDayofMonth\nDayOfWeek\nDOT_ID_Reporting_Airline\nFlight_Number_Reporting_Airline\nOriginAirportID\nOriginAirportSeqID\nOriginCityMarketID\nOriginStateFips\nOriginWac\nDestAirportID\nDestAirportSeqID\nDestCityMarketID\nDestStateFips\nDestWac\nCRSDepTime\nDepartureDelayGroups\nCRSArrTime\nArrivalDelayGroups\nDistanceGroup\nDivAirportLandings\n\n\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\n1\n1\n14\n5\n20452\n4879\n11066\n1106606\n31066\n39\n44\n11278\n1127805\n30852\n51\n38\n1224\n-1\n1352\n0\n2\n0\n\n\n1\n1\n15\n6\n20452\n4879\n11066\n1106606\n31066\n39\n44\n11278\n1127805\n30852\n51\n38\n1224\n-1\n1352\n-2\n2\n0\n\n\n1\n1\n16\n7\n20452\n4879\n11066\n1106606\n31066\n39\n44\n11278\n1127805\n30852\n51\n38\n1224\n-1\n1352\n-1\n2\n0\n\n\n\n\n\n\n\n# select all numeric columns\nfull_flights.select(\n    pl.col(pl.Int64),\n    pl.col(pl.Float64)\n).head(3)\n\n\nshape: (3, 44)\n\n\n\nQuarter\nMonth\nDayofMonth\nDayOfWeek\nDOT_ID_Reporting_Airline\nFlight_Number_Reporting_Airline\nOriginAirportID\nOriginAirportSeqID\nOriginCityMarketID\nOriginStateFips\nOriginWac\nDestAirportID\nDestAirportSeqID\nDestCityMarketID\nDestStateFips\nDestWac\nCRSDepTime\nDepartureDelayGroups\nCRSArrTime\nArrivalDelayGroups\nDistanceGroup\nDivAirportLandings\nDepDelay\nDepDelayMinutes\nDepDel15\nTaxiOut\nTaxiIn\nArrDelay\nArrDelayMinutes\nArrDel15\nCancelled\nDiverted\nCRSElapsedTime\nActualElapsedTime\nAirTime\nFlights\nDistance\nCarrierDelay\nWeatherDelay\nNASDelay\nSecurityDelay\nLateAircraftDelay\nTotalAddGTime\nLongestAddGTime\n\n\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1\n1\n14\n5\n20452\n4879\n11066\n1106606\n31066\n39\n44\n11278\n1127805\n30852\n51\n38\n1224\n-1\n1352\n0\n2\n0\n-3.0\n0.0\n0.0\n28.0\n4.0\n4.0\n4.0\n0.0\n0.0\n0.0\n88.0\n95.0\n63.0\n1.0\n323.0\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n1\n1\n15\n6\n20452\n4879\n11066\n1106606\n31066\n39\n44\n11278\n1127805\n30852\n51\n38\n1224\n-1\n1352\n-2\n2\n0\n-10.0\n0.0\n0.0\n19.0\n5.0\n-24.0\n0.0\n0.0\n0.0\n0.0\n88.0\n74.0\n50.0\n1.0\n323.0\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n1\n1\n16\n7\n20452\n4879\n11066\n1106606\n31066\n39\n44\n11278\n1127805\n30852\n51\n38\n1224\n-1\n1352\n-1\n2\n0\n-6.0\n0.0\n0.0\n16.0\n12.0\n-13.0\n0.0\n0.0\n0.0\n0.0\n88.0\n81.0\n53.0\n1.0\n323.0\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n\n\n\n\n\nComplex column selection with Selectors, analogous to dplyr’s selection helpers\n\n\nimport polars.selectors as cs\n\n# match all variables. Same as dplyr's `everything()`\nfull_flights.select(cs.all()).head(3)\n\n\nshape: (3, 110)\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\nFlightDate\nReporting_Airline\nDOT_ID_Reporting_Airline\nIATA_CODE_Reporting_Airline\nTail_Number\nFlight_Number_Reporting_Airline\nOriginAirportID\nOriginAirportSeqID\nOriginCityMarketID\nOrigin\nOriginCityName\nOriginState\nOriginStateFips\nOriginStateName\nOriginWac\nDestAirportID\nDestAirportSeqID\nDestCityMarketID\nDest\nDestCityName\nDestState\nDestStateFips\nDestStateName\nDestWac\nCRSDepTime\nDepTime\nDepDelay\nDepDelayMinutes\nDepDel15\nDepartureDelayGroups\nDepTimeBlk\nTaxiOut\n…\nDiv1TotalGTime\nDiv1LongestGTime\nDiv1WheelsOff\nDiv1TailNum\nDiv2Airport\nDiv2AirportID\nDiv2AirportSeqID\nDiv2WheelsOn\nDiv2TotalGTime\nDiv2LongestGTime\nDiv2WheelsOff\nDiv2TailNum\nDiv3Airport\nDiv3AirportID\nDiv3AirportSeqID\nDiv3WheelsOn\nDiv3TotalGTime\nDiv3LongestGTime\nDiv3WheelsOff\nDiv3TailNum\nDiv4Airport\nDiv4AirportID\nDiv4AirportSeqID\nDiv4WheelsOn\nDiv4TotalGTime\nDiv4LongestGTime\nDiv4WheelsOff\nDiv4TailNum\nDiv5Airport\nDiv5AirportID\nDiv5AirportSeqID\nDiv5WheelsOn\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\nstr\ni64\ni64\ni64\ni64\nstr\nstr\ni64\nstr\nstr\ni64\ni64\ni64\ni64\nstr\nstr\nstr\ni64\nstr\ni64\ni64\ni64\ni64\nstr\nstr\nstr\ni64\nstr\ni64\ni64\nstr\nf64\nf64\nf64\ni64\nstr\nf64\n…\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"2022\"\n1\n1\n14\n5\n\"2022-01-14\"\n\"YX\"\n20452\n\"YX\"\n\"N119HQ\"\n4879\n11066\n1106606\n31066\n\"CMH\"\n\"Columbus, OH\"\n\"OH\"\n39\n\"Ohio\"\n44\n11278\n1127805\n30852\n\"DCA\"\n\"Washington, DC…\n\"VA\"\n51\n\"Virginia\"\n38\n1224\n\"1221\"\n-3.0\n0.0\n0.0\n-1\n\"1200-1259\"\n28.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\"2022\"\n1\n1\n15\n6\n\"2022-01-15\"\n\"YX\"\n20452\n\"YX\"\n\"N122HQ\"\n4879\n11066\n1106606\n31066\n\"CMH\"\n\"Columbus, OH\"\n\"OH\"\n39\n\"Ohio\"\n44\n11278\n1127805\n30852\n\"DCA\"\n\"Washington, DC…\n\"VA\"\n51\n\"Virginia\"\n38\n1224\n\"1214\"\n-10.0\n0.0\n0.0\n-1\n\"1200-1259\"\n19.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\"2022\"\n1\n1\n16\n7\n\"2022-01-16\"\n\"YX\"\n20452\n\"YX\"\n\"N412YX\"\n4879\n11066\n1106606\n31066\n\"CMH\"\n\"Columbus, OH\"\n\"OH\"\n39\n\"Ohio\"\n44\n11278\n1127805\n30852\n\"DCA\"\n\"Washington, DC…\n\"VA\"\n51\n\"Virginia\"\n38\n1224\n\"1218\"\n-6.0\n0.0\n0.0\n-1\n\"1200-1259\"\n16.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\n# match variables by prefix or suffix\nfull_flights.select(\n    cs.starts_with('Flight') | cs.ends_with('Delay')\n).head(3)\n\n\nshape: (3, 11)\n\n\n\nFlightDate\nFlight_Number_Reporting_Airline\nFlights\nDepDelay\nArrDelay\nCarrierDelay\nWeatherDelay\nNASDelay\nSecurityDelay\nLateAircraftDelay\nDivArrDelay\n\n\nstr\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"2022-01-14\"\n4879\n1.0\n-3.0\n4.0\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n\"2022-01-15\"\n4879\n1.0\n-10.0\n-24.0\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n\"2022-01-16\"\n4879\n1.0\n-6.0\n-13.0\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n\n\n\n\nA detailed comparison between dplyr and polars column selection helpers:\n\n\n\n\n\n\n\n\ndplyr action\ndplyr function\npolars function\n\n\n\n\nmatches all columns\neverything()\ncs.all()\n\n\nmatches all integer columns\nwhere(is.integer())\ncs.integer()\n\n\nmatches all factor/categorical columns\nwhere(is.factor())\ncs.categorical()\n\n\nmatches all float columns\nwhere(is.double())\ncs.float()\n\n\nmatches all string columns\nwhere(is.character())\ncs.string()\n\n\nmatches all date columns\nwhere(\\(x) class(x) == \"Date\")\ncs.date()\n\n\nmatch all datetime columns\nnot available\ncs.datetime()\n\n\nmatch all time columns\nnot available\ncs.time()\n\n\nmatch all date/datetime columns\nnot available\ncs.temporal()\n\n\nselect the last column\nlast_col()\ncs.last()\n\n\nstarts with a prefix\nstarts_with()\ncs.starts_with()\n\n\nends with a suffix\nend_with()\ncs.ends_with()\n\n\ncontains a literal string\ncontains()\ncs.contains()\n\n\nmatches a regular expression\nmatches()\ncs.matches()\n\n\ntakes the complement of a set of columns\n!\n~\n\n\ntakes the difference between a set of columns\nnot available\n-\n\n\nselects a range of consecutive columns\ncol1:col10\nnot available\n\n\nmatches all column names in a list, no error thrown for non-existing names\nany_of()\nnot available\n\n\n\n\n\n2.3.3 Modify/Add column\n\n\n2.3.4 Rename column\n\n\n2.3.5 Remove column\n\n\n2.3.6 Aggregation by group\n\n\n2.3.7 Sort a DataFrame",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#two-tables-operation",
    "href": "data_manipulation.html#two-tables-operation",
    "title": "2  Data manipulation",
    "section": "2.4 Two tables operation",
    "text": "2.4 Two tables operation\n\n2.4.1 Join DataFrames\n\n\n2.4.2 Concatenate DataFrames",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#pivot-a-dataframe",
    "href": "data_manipulation.html#pivot-a-dataframe",
    "title": "2  Data manipulation",
    "section": "2.5 Pivot a DataFrame",
    "text": "2.5 Pivot a DataFrame",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#dealing-with-missing-values",
    "href": "data_manipulation.html#dealing-with-missing-values",
    "title": "2  Data manipulation",
    "section": "2.6 Dealing with missing values",
    "text": "2.6 Dealing with missing values",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#strings-methods",
    "href": "data_manipulation.html#strings-methods",
    "title": "2  Data manipulation",
    "section": "2.7 Strings methods",
    "text": "2.7 Strings methods",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#handling-datetime",
    "href": "data_manipulation.html#handling-datetime",
    "title": "2  Data manipulation",
    "section": "2.8 Handling datetime",
    "text": "2.8 Handling datetime",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#other-useful-methods",
    "href": "data_manipulation.html#other-useful-methods",
    "title": "2  Data manipulation",
    "section": "2.9 Other useful methods",
    "text": "2.9 Other useful methods",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "import_export.html",
    "href": "import_export.html",
    "title": "3  Reading and writing data with Polars",
    "section": "",
    "text": "3.1 Reading data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and writing data with Polars</span>"
    ]
  },
  {
    "objectID": "import_export.html#reading-data",
    "href": "import_export.html#reading-data",
    "title": "3  Reading and writing data with Polars",
    "section": "",
    "text": "3.1.1 Delimited file (CSV, TSV)\nFor a complete list of parameter options to use with Polars CSV readers, see this page. These parameters provides the same functionality as readr’s arguments but with slightly different names. Here’s an example:\n\n# Read a CSV file\nflights_202212 = pl.read_csv(\n    source='./data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_12.csv',\n    separator=',',                  # single character used to separate fields, default=True\n    has_header=True,                # flag indicating whether the first row contains header, default=True\n    infer_schema_length=1000,       # maximum number of lines to read for schema inference, default=100\n    n_rows=10                       # maximum number of lines to read\n)\n\n# Display the first 3 rows\nflights_202212.head(3)\n\n\nshape: (3, 110)\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\n…\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\ni64\ni64\ni64\ni64\ni64\n…\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n2022\n4\n12\n19\n1\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n4\n12\n20\n2\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n4\n12\n21\n3\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\nIn a complex data set, it is common to override data types for specific columns\n\n# Read the CSV file with specified data types for selected columns\nflights_202212 = pl.read_csv(\n    source='./data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_12.csv',\n    separator=',',                  # single character used to separate fields, default=True\n    has_header=True,                # flag the first row has header or not, default=True\n    infer_schema_length=1000,       # maximum number of lines to read to infer schema, default=100\n    n_rows=10,                      # maximum number of lines to read\n    try_parse_dates=True, \n    dtypes={'Year':pl.Int32, 'Quarter':pl.Int32, 'Month': pl.Int32, 'Reporting_Airline': pl.Categorical} \n)\n\nflights_202212.head(3)\n\n\nshape: (3, 110)\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\n…\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\ni32\ni32\ni32\ni64\ni64\n…\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n2022\n4\n12\n19\n1\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n4\n12\n20\n2\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n4\n12\n21\n3\n…\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\n\n3.1.2 Reading multiple files\nPolars’ scan_*() method is a really neat technique for reading multiple files efficiently.\n\nfrom pathlib import Path\n\ndef convert_bytes(size):\n    \"\"\" Convert bytes to KB, or MB or GB\"\"\"\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if size &lt; 1024.0:\n            return \"%3.1f %s\" % (size, x)\n        size /= 1024.0\n\nfor p in Path.cwd().rglob('data/On_Time*.csv'):\n    print(p.name, \":\", convert_bytes(p.stat().st_size))\n\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv : 249.1 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv : 229.9 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_8.csv : 253.9 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_6.csv : 248.4 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_7.csv : 256.5 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_3.csv : 243.3 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_2.csv : 212.4 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_12.csv : 239.6 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_4.csv : 239.5 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_9.csv : 239.5 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_10.csv : 246.6 MB\nOn_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_11.csv : 235.7 MB\n\n\nTo read multiple files into a single DataFrame, we can use globbing patterns:\n\n%%time\nno_flights_each_month = (\n    pl.scan_csv('./data/On_Time*.csv')\n    .group_by(['Year', 'Month'])\n    .agg(\n        pl.len().alias('Rows Count')\n    )\n    .sort('Month')\n    .collect()\n)\n\nno_flights_each_month\n\nIf your files don’t have to be in a single table you can also build a query plan for each file and execute them in parallel on the Polars thread pool.\nAll query plan execution is embarrassingly parallel and doesn’t require any communication.\n\nqueries = []\nfor p in Path.cwd().rglob('data/On_Time*.csv'):\n    q = (\n        pl.scan_csv(p)\n        .group_by(['Year', 'Month'])\n        .agg(\n            pl.len().alias('Rows Count')\n        )\n    )\n    queries.append(q)\n\ndfs = pl.collect_all(queries)\ndfs[:3]\n\n\n\n3.1.3 Reading spreadsheets\n\n\n3.1.4 Reading a SQL table\n\n\n3.1.5 Reading S3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and writing data with Polars</span>"
    ]
  },
  {
    "objectID": "import_export.html#writing-data",
    "href": "import_export.html#writing-data",
    "title": "3  Reading and writing data with Polars",
    "section": "3.2 Writing data",
    "text": "3.2 Writing data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and writing data with Polars</span>"
    ]
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "5  Polars in action",
    "section": "",
    "text": "5.1 Why Pandas feels clunky after using R, but Polars doesn’t\nThe initial demonstration of Polars in action isn’t a real-world scenario but rather a simplified example, deliberately chosen to highlight Polars’ fluent API, which, in my view, resonates with many tidyverse users.\nRasmus Baath, in his insightful blog post, compared the usability of R’s dplyr and Python’s Pandas using a simple data analysis exercise. He argued that Pandas feels clunky and complicated compared to the core package of the tidyverse. I found his example to be complelling because I have had the same personal experience using Pandas for various data tasks at work.\nBy borrowing Ramus’s case, I aim to showcase Polar’s fluent interface which is arguably as silky smooth as tidyverse’s. While this example carries biased opinion, it serves as an useful illustration of what a good API design should look like. Pandas remains an excellent package, but it’s important to acknowledge its quirks, especially for newcomers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars in action</span>"
    ]
  },
  {
    "objectID": "examples.html#why-pandas-feels-clunky-after-using-r-but-polars-doesnt",
    "href": "examples.html#why-pandas-feels-clunky-after-using-r-but-polars-doesnt",
    "title": "5  Polars in action",
    "section": "",
    "text": "5.1.1 Reading data\n\n\n\n\n# R\nrequire(readr)\npurchases = readr::read_csv(\"./data/purchases.csv\")\npurchases |&gt; head()\n\n# A tibble: 6 × 3\n  country amount discount\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 USA       2000       10\n2 USA       3500       15\n3 USA       3000       20\n4 Canada     120       12\n5 Canada     180       18\n6 Canada    3100       21\n\n\n\n\n\n\n# Pandas\nimport pandas as pd\npurchases_pd = pd.read_csv(\"./data/purchases.csv\")\npurchases_pd.head()\n\n  country  amount  discount\n0     USA    2000        10\n1     USA    3500        15\n2     USA    3000        20\n3  Canada     120        12\n4  Canada     180        18\n\n\n\n\n\n\n# Polars\nimport polars as pl\npurchases_pl = pl.read_csv(\"./data/purchases.csv\")\npl.Config.set_tbl_formatting('NOTHING')\n\n&lt;class 'polars.config.Config'&gt;\n\npl.Config.set_tbl_hide_column_data_types(True)\n\n&lt;class 'polars.config.Config'&gt;\n\nprint(purchases_pl.head())\n\nshape: (5, 3)\n country  amount  discount \n USA      2000    10       \n USA      3500    15       \n USA      3000    20       \n Canada   120     12       \n Canada   180     18       \n\n\n\n\n\n\n\n5.1.2 “How much do we sell..? Let’s take the total sum!”\n\n\n\n\n# R\npurchases$amount |&gt; sum()\n\n[1] 17210\n\n\n\n\n\n\n# Pandas\npurchases_pd[\"amount\"].sum()\n\n17210\n\n\n\n\n\n\n# Polars\npurchases_pl['amount'].sum()\n\n17210\n\n\n\n\n\n\n\n5.1.3 “Ah, they wanted it by country…”\n\n\n\n\n# R\npurchases |&gt;\n  dplyr::group_by(country) |&gt;\n  dplyr::summarize(total = sum(amount))\n\n# A tibble: 11 × 2\n   country   total\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Australia   600\n 2 Brazil      460\n 3 Canada     3400\n 4 France      500\n 5 Germany     570\n 6 India       720\n 7 Italy       630\n 8 Japan       690\n 9 Spain       660\n10 UK          480\n11 USA        8500\n\n\n\n\n\n\n# Pandas\n(purchases_pd\n  .groupby(\"country\")\n  .agg(total=(\"amount\", \"sum\")) \n  .reset_index()                \n)\n\n      country  total\n0   Australia    600\n1      Brazil    460\n2      Canada   3400\n3      France    500\n4     Germany    570\n5       India    720\n6       Italy    630\n7       Japan    690\n8       Spain    660\n9          UK    480\n10        USA   8500\n\n\n\n\n\n\n# Polars\nprint(purchases_pl\n  .group_by('country')\n  .agg(pl.col('amount').sum().alias('total'))\n  .sort('country')\n)\n\nshape: (11, 2)\n country    total \n Australia  600   \n Brazil     460   \n Canada     3400  \n France     500   \n Germany    570   \n India      720   \n Italy      630   \n Japan      690   \n Spain      660   \n UK         480   \n USA        8500  \n\n\n\n\n\n\n\n5.1.4 “And I guess I should deduct the discount.”\n\n\n\n\n# R\npurchases |&gt; \n  dplyr::group_by(country) |&gt; \n  dplyr::summarize(total = sum(amount - discount))\n\n# A tibble: 11 × 2\n   country   total\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Australia   540\n 2 Brazil      414\n 3 Canada     3349\n 4 France      450\n 5 Germany     513\n 6 India       648\n 7 Italy       567\n 8 Japan       621\n 9 Spain       594\n10 UK          432\n11 USA        8455\n\n\n\n\n\n\n# Pandas\n(purchases_pd\n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})                           \n)\n\n      country  total\n0   Australia    540\n1      Brazil    414\n2      Canada   3349\n3      France    450\n4     Germany    513\n5       India    648\n6       Italy    567\n7       Japan    621\n8       Spain    594\n9          UK    432\n10        USA   8455\n\n\n\n\n\n\n# Polars\nprint(purchases_pl\n  .group_by('country')\n  .agg(\n    (pl.col('amount') - pl.col('discount')).sum().alias('total')\n  )\n  .sort('country')\n)\n\nshape: (11, 2)\n country    total \n Australia  540   \n Brazil     414   \n Canada     3349  \n France     450   \n Germany    513   \n India      648   \n Italy      567   \n Japan      621   \n Spain      594   \n UK         432   \n USA        8455  \n\n\n\n\n\n\n\n5.1.5 “Oh, and Maria asked me to remove any outliers.”\n\n\n\n\n# R\npurchases |&gt;\n  dplyr::filter(amount &lt;= median(amount) * 10) |&gt;\n  dplyr::group_by(country) |&gt; \n  dplyr::summarize(total = sum(amount - discount))\n\n# A tibble: 11 × 2\n   country   total\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Australia   540\n 2 Brazil      414\n 3 Canada      270\n 4 France      450\n 5 Germany     513\n 6 India       648\n 7 Italy       567\n 8 Japan       621\n 9 Spain       594\n10 UK          432\n11 USA        1990\n\n\n\n\n\n\n# Pandas\n(purchases_pd\n  .query(\"amount &lt;= amount.median() * 10\")\n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n)\n\n      country  total\n0   Australia    540\n1      Brazil    414\n2      Canada    270\n3      France    450\n4     Germany    513\n5       India    648\n6       Italy    567\n7       Japan    621\n8       Spain    594\n9          UK    432\n10        USA   1990\n\n\n\n\n\n\n# Polars\nprint(purchases_pl\n  .filter(\n    pl.col('amount') &lt;= (pl.col('amount').median() * 10)\n  )\n  .group_by('country')\n  .agg(\n    (pl.col('amount') - pl.col('discount')).sum().alias('total')\n  )\n  .sort('country')\n)\n\nshape: (11, 2)\n country    total \n Australia  540   \n Brazil     414   \n Canada     270   \n France     450   \n Germany    513   \n India      648   \n Italy      567   \n Japan      621   \n Spain      594   \n UK         432   \n USA        1990  \n\n\n\n\n\n\n\n5.1.6 “I probably should use the median within each country”\n\n\n\n\n# R \npurchases |&gt;\n  dplyr::group_by(country) |&gt;                     \n  dplyr::filter(amount &lt;= median(amount) * 10) |&gt; \n  dplyr::summarize(total = sum(amount - discount))\n\n# A tibble: 11 × 2\n   country   total\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Australia   540\n 2 Brazil      414\n 3 Canada      270\n 4 France      450\n 5 Germany     513\n 6 India       648\n 7 Italy       567\n 8 Japan       621\n 9 Spain       594\n10 UK          432\n11 USA        8455\n\n\n\n\n\n\n# Pandas\n(purchases_pd\n  .groupby(\"country\")                                               \n  .apply(lambda df: df[df[\"amount\"] &lt;= df[\"amount\"].median() * 10]) \n  .reset_index(drop=True)                                           \n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n)\n\n      country  total\n0   Australia    540\n1      Brazil    414\n2      Canada    270\n3      France    450\n4     Germany    513\n5       India    648\n6       Italy    567\n7       Japan    621\n8       Spain    594\n9          UK    432\n10        USA   8455\n\n\n\n\n\n\n# Polars\nprint(purchases_pl\n  .filter(\n    pl.col('amount') &lt;= (pl.col('amount').median().over('country') * 10)\n  )\n  .group_by('country')\n  .agg(\n    (pl.col('amount') - pl.col('discount')).sum().alias('total')\n  )\n  .sort('country')\n)\n\nshape: (11, 2)\n country    total \n Australia  540   \n Brazil     414   \n Canada     270   \n France     450   \n Germany    513   \n India      648   \n Italy      567   \n Japan      621   \n Spain      594   \n UK         432   \n USA        8455",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars in action</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Summary & references",
    "section": "",
    "text": "Summary",
    "crumbs": [
      "Summary & references"
    ]
  },
  {
    "objectID": "references.html#summary",
    "href": "references.html#summary",
    "title": "Summary & references",
    "section": "",
    "text": "Zen of polars (a.k.a. the polars design principles). If you’re into tidyverse, you might find the polars API design guideline familiar with tidy design principles\n\n        - readability over saving keystrokes\n        - explicit over implicit\n        - aim for a single return dtype per expression\n        - API should nudge to fast code\n        - pure over in-place\n        - undersore over concatenated words\n        - minimize ambiguity",
    "crumbs": [
      "Summary & references"
    ]
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "Summary & references",
    "section": "References",
    "text": "References\nHere is a compilation of resources, including talks, articles, blogs and tutorials, that I studied in crafting this book.\n\nMust watch: Ritchie Vink’s (author of polars) keynote at EuroSciPy 2023\nPolars user guide\nModern polars\nCookbook polars for R\nPython’s polars API reference\nPractical business Python - Introduction to polars\nReal Python - Python polars: A lightning-fast DataFrame libray\nA bird’s eye view of polars\nAwesome polars - A curated list of polars docs, talks, tools, examples and articles",
    "crumbs": [
      "Summary & references"
    ]
  }
]